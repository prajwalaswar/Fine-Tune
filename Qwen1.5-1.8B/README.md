# ğŸ§  Fine-Tune Qwen 1.5 (1.8B) with QLoRA on Google Colab

This project shows how to fine-tune **Alibabaâ€™s Qwen 1.5 (1.8B)** model using **QLoRA (Parameter-Efficient Fine-Tuning)** on the Hugging Face dataset `timdettmers/openassistant-guanaco`. The notebook is designed to run on **Google Colab Free Tier** with a GPU.

---

## ğŸš€ Features

- âœ… Load Qwen 1.5 (1.8B) model and tokenizer
- âœ… Use PEFT (QLoRA) for efficient training
- âœ… Use small HF dataset to stay within Colab limits
- âœ… Save fine-tuned model
- âœ… Perform inference inside the notebook

---

## ğŸ› ï¸ Tech Stack

- ğŸ¤— Hugging Face Transformers
- ğŸ¤— PEFT (QLoRA)
- ğŸ¤— Datasets
- ğŸ¤— Accelerate
- Google Colab (Free GPU)

---

## ğŸ“‚ Dataset

- **Name:** [`timdettmers/openassistant-guanaco`](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
- **Type:** Instruction â†’ Response format
- **Purpose:** Train model to follow human-like instructions

---

## ğŸ“‹ Setup Instructions

### 1. Open Google Colab
- Go to: [https://colab.research.google.com](https://colab.research.google.com)
- Make sure GPU is enabled (`Runtime` â†’ `Change runtime type` â†’ select `GPU`)

### 2. Run Notebook Step-by-Step
- Install libraries
- Load model and dataset
- Apply QLoRA using `peft`
- Train model
- Run inference
- Save the fine-tuned model

---

## ğŸ§ª Sample Inference

```python
prompt = "### Instruction:\nWhat is QLoRA?\n\n### Response:\n"
output = pipe(prompt, max_new_tokens=100, temperature=0.7)
print(output[0]["generated_text"])
